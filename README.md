# Cyberbullying Detection using Python and Machine Learning - Complete Tutorial Summary

## Overview
This tutorial demonstrates building a **machine learning system to detect cyberbullying** from social media text (Twitter/Facebook/Instagram). The project uses Natural Language Processing (NLP), multiple classification algorithms, and deployment via Flask web application.

## Problem Statement
**Goal:** Automatically detect bullying vs non-bullying text from social media posts

**Application:** Can be integrated into social platforms to flag harmful content automatically

---

## DEMO APPLICATION

### Web Interface Features
- **Input:** Text area for user to enter text
- **Button:** "Analyze" button to process text
- **Output:** 
  - Red signal: "Bullying" (harmful content detected)
  - Green signal: "Non-Bullying" (safe content)

**Example Tests:**
- Input: Random bullying text â†’ Output: "Bullying"
- Input: "He is a good boy" â†’ Output: "Non-Bullying"

---

## DEVELOPMENT ENVIRONMENT

### Platform
- **IDE:** Jupyter Notebook (Google Colab/PyCharm also compatible)
- **Language:** Python 3.x

### Libraries Required

**Data Processing:**
```python
import pandas as pd
import numpy as np
```

**Visualization:**
```python
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')
```

**NLP Libraries:**
```python
import re  # Regular expressions
import nltk
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
```

**Machine Learning:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer
```

**Classification Algorithms:**
```python
import xgboost as xgb
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
```

**Model Persistence:**
```python
import pickle
import time
```

---

## DATASET

### Structure
**File:** dataset.csv

**Columns:**
1. **Headline** - Text content (tweets/posts)
2. **Label** - Classification
   - `-1` = Toxic (Bullying)
   - `0` = Non-Toxic (Non-Bullying)

**Records:** 18,144 text samples

### Initial Exploration

```python
df = pd.read_csv('dataset.csv')
df.head()
```

**Label Distribution Check:**
```python
df['label'].unique()
# Output: [-1, 0]
```

---

## DATA PREPROCESSING

### Step 1: Data Manipulation

**Problem:** Labels are -1 and 0 (need 1 and 0 for binary classification)

**Solution:** Convert -1 to 1

```python
def perform_data_manipulation():
    df = pd.read_csv('dataset.csv')
    
    for index in df.index:
        if df.loc[index, 'label'] == -1:
            df.loc[index, 'label'] = 1
    
    return df

df = perform_data_manipulation()
```

**Verification:**
```python
df['label'].unique()
# Output: [1, 0]
```

---

### Step 2: Data Distribution Analysis

**Purpose:** Check if dataset is balanced or imbalanced

**Pie Chart Visualization:**

```python
def perform_data_distribution(df):
    total = df.shape[0]  # Total rows
    
    # Count non-toxic samples
    num_non_toxic = df[df['label'] == 0].shape[0]
    
    # Calculate slices
    slices = [
        num_non_toxic / total,
        (total - num_non_toxic) / total
    ]
    
    # Labels
    labels = ['Non-Toxic', 'Toxic']
    
    # Explode effect
    explode = [0.2, 0]
    
    # Create pie chart
    plt.pie(
        slices,
        explode=explode,
        shadow=True,
        autopct='%1.1f%%',
        labels=labels,
        wedgeprops={'edgecolor': 'black'}
    )
    
    plt.title('Number of Toxic vs Non-Toxic Test Samples')
    plt.tight_layout()
    plt.show()

perform_data_distribution(df)
```

**Result:**
- Non-Toxic: 35.7%
- Toxic: 64.3%

**Interpretation:** Slight imbalance, but acceptable (no rebalancing needed)

---

### Step 3: Remove Pattern Function

**Purpose:** Clean text by removing unwanted patterns

```python
def remove_pattern(input_text, pattern):
    if type(input_text) == str:
        # Find all matches
        r = re.findall(pattern, input_text)
        
        # Remove each match
        for i in r:
            input_text = re.sub(i, '', input_text)
        
        return input_text
    else:
        return ''
```

**Usage:** Removes @ mentions, hashtags, special characters

---

### Step 4: Dataset Cleaning Function

**Comprehensive text cleaning pipeline:**

```python
def dataset_cleaning(df):
    # 1. Calculate headline length
    length_headline = df['headline'].str.len()
    
    # 2. Combine into dataframe
    combined_df = df.append(
        pd.DataFrame({'len_headline': length_headline}),
        ignore_index=True
    )
    
    # 3. Remove @user mentions
    tidy_tweet = np.vectorize(remove_pattern)(
        combined_df['headline'],
        r"@[\w]*"  # Pattern: @ followed by word characters
    )
    
    # 4. Remove extra letters/spaces
    tidy_tweet = combined_df['headline'].str.replace(
        "[^a-zA-Z#]",  # Keep only letters and #
        " "
    )
    
    # 5. Remove words less than 3 characters
    tidy_tweet = tidy_tweet.apply(
        lambda x: ' '.join([w for w in x.split() if len(w) > 3])
    )
    
    # 6. Calculate cleaned text length
    length_tidy = tidy_tweet.str.len()
    combined_df['len_tidy'] = length_tidy
    
    # 7. Tokenize (create list)
    tokenized_tweet = tidy_tweet.apply(lambda x: x.split())
    
    # 8. Lemmatization
    nltk.download('wordnet')
    lemmatizer = WordNetLemmatizer()
    
    tokenized_tweet = tokenized_tweet.apply(
        lambda x: [lemmatizer.lemmatize(i) for i in x]
    )
    
    # 9. Join back to strings
    for i in range(len(tokenized_tweet)):
        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])
    
    combined_df['tidy_tweet'] = tokenized_tweet
    
    return combined_df, df

combined_df, df = dataset_cleaning(df)
```

**Cleaning Steps Summary:**
1. Calculate original text length
2. Remove @ user mentions
3. Remove special characters (keep letters and #)
4. Remove words with < 3 characters
5. Tokenize into word lists
6. Lemmatize words (reduce to root form)
7. Join back into cleaned strings

**Example Transformation:**
- **Before:** "@user This is #bad!!! I hate you so much!!!"
- **After:** "hate much"

---

### Step 5: Train-Test Split

```python
from sklearn.model_selection import train_test_split

def perform_data_split(test_size, random_state, combined_df, df):
    # Define X and y
    X = combined_df['tidy_tweet']
    y = combined_df['label']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state
    )
    
    # Print statistics
    print(f"Number of rows in total dataset: {combined_df.shape[0]}")
    print(f"Number of rows in training dataset: {X_train.shape[0]}")
    print(f"Number of rows in test dataset: {X_test.shape[0]}")
    
    return X_train, X_test, y_train, y_test

# Execute split (80-20)
X_train, X_test, y_train, y_test = perform_data_split(
    test_size=0.20,
    random_state=42,
    combined_df=combined_df,
    df=df
)
```

**Split Configuration:**
- **Training:** 80% (~14,515 samples)
- **Testing:** 20% (~3,629 samples)

---

### Step 6: TF-IDF Vectorization

**Purpose:** Convert text to numerical features

**Load Stop Words:**
```python
# Load custom stopwords file
with open('stopwords.txt', 'r') as file:
    content = file.read()
    content_list = content.split('\n')
    file.close()
```

**Create TF-IDF Vectorizer:**
```python
tfidf_vectorizer = TfidfVectorizer(
    stop_words=content_list,
    lowercase=True
)

# Fit on training data
training_data = tfidf_vectorizer.fit_transform(
    X_train.values.astype('U')
).astype('uint')

# Transform test data
testing_data = tfidf_vectorizer.transform(
    X_test.values.astype('U')
).astype('uint')
```

**Save Vectorizer:**
```python
filename = 'tfidf_vectorizer.pkl'

pickle.dump(
    tfidf_vectorizer.vocabulary_,
    open(filename, 'wb')
)
```

**What is TF-IDF?**

**TF (Term Frequency):**
```
TF = (Word count in document) / (Total words in document)
```

**IDF (Inverse Document Frequency):**
```
IDF = log(Total documents / Documents containing word)
```

**TF-IDF:**
```
TF-IDF = TF Ã— IDF
```

**Why use TF-IDF?**
- Highlights important words
- Reduces weight of common words
- Better than simple word counts
- Creates sparse matrix (memory efficient)

---

## MODEL TRAINING

### Multiple Algorithm Pipeline

**Models Tested:**
1. LinearSVC
2. LogisticRegression
3. MultinomialNB (Naive Bayes)
4. DecisionTreeClassifier
5. AdaBoostClassifier
6. BaggingClassifier
7. SGDClassifier

**Training Pipeline Function:**

```python
def pipeline(X_train, y_train, X_test, y_test):
    # Define models
    models = [
        LinearSVC(),
        LogisticRegression(),
        MultinomialNB(),
        DecisionTreeClassifier(),
        AdaBoostClassifier(),
        BaggingClassifier(),
        SGDClassifier()
    ]
    
    final_results = []
    
    for model in models:
        # Store results
        results = {
            'Algorithm': model.__class__.__name__
        }
        
        # Training
        start = time.time()
        model.fit(X_train, y_train)
        end = time.time()
        
        # Save model
        filename = f"{model.__class__.__name__}.pkl"
        pickle.dump(model, open(filename, 'wb'))
        
        # Store training time
        results['Training Time'] = end - start
        
        # Predictions
        start = time.time()
        prediction_test = model.predict(X_test)
        prediction_train = model.predict(X_train)
        end = time.time()
        
        # Store prediction time
        results['Prediction Time'] = end - start
        
        # Test metrics
        results['Accuracy Test'] = accuracy_score(y_test, prediction_test)
        results['F1 Test'] = f1_score(y_test, prediction_test)
        results['Precision Test'] = precision_score(y_test, prediction_test)
        results['Recall Test'] = recall_score(y_test, prediction_test)
        
        # Train metrics
        results['Accuracy Train'] = accuracy_score(y_train, prediction_train)
        results['F1 Train'] = f1_score(y_train, prediction_train)
        results['Precision Train'] = precision_score(y_train, prediction_train)
        results['Recall Train'] = recall_score(y_train, prediction_train)
        
        # Print progress
        print(f"Finished training {model.__class__.__name__} in {results['Training Time']:.2f} seconds")
        
        # Append to results
        final_results.append(results.copy())
    
    return final_results

# Execute training
final_results = pipeline(training_data, y_train, testing_data, y_test)
```

---

### Model Performance Results

**Training Times:**
- LinearSVC: 0.34 seconds âš¡ (Fastest)
- LogisticRegression: 3.53 seconds
- MultinomialNB: 0.10 seconds âš¡âš¡ (Fastest)
- DecisionTreeClassifier: 80 seconds ðŸŒ
- AdaBoostClassifier: 8.62 seconds
- BaggingClassifier: 182 seconds ðŸŒðŸŒ (Slowest)
- SGDClassifier: Fast

**Accuracy Comparison:**
```
Algorithm              | Test Acc | F1 Test | Training Time
--------------------- | -------- | ------- | -------------
DecisionTreeClassifier | Highest  | ~98%    | 80s
BaggingClassifier     | High     | ~97%    | 182s
LinearSVC             | Good     | ~93%    | 0.34s
LogisticRegression    | Good     | ~92%    | 3.53s
SGDClassifier         | Good     | ~91%    | Fast
MultinomialNB         | Lower    | ~88%    | 0.10s
AdaBoostClassifier    | Moderate | ~90%    | 8.62s
```

---

### Results Analysis Function

```python
def perform_final_results(final_results):
    # Create DataFrame
    results = pd.DataFrame(final_results)
    
    # Reindex columns
    results = results.reindex(columns=[
        'Algorithm',
        'Accuracy Test',
        'Precision Test',
        'Recall Test',
        'F1 Test',
        'Accuracy Train',
        'Precision Train',
        'Recall Train',
        'F1 Train',
        'Training Time',
        'Prediction Time'
    ])
    
    # Sort by F1 score (descending)
    results = results.sort_values(
        by='F1 Test',
        ascending=False,
        inplace=False
    )
    
    # Reset index
    results = results.reset_index(drop=True)
    
    return results

results = perform_final_results(final_results)
results.head(10)
```

**Descriptive Statistics:**
```python
results.describe().loc[['min', 'max']]
```

**Output:**
- Maximum training time: 182 seconds (BaggingClassifier)
- Minimum training time: 0.10 seconds (MultinomialNB)

---

### Visualization: Algorithm Performance

**Test Accuracy and F1 Score Comparison:**

```python
# Plot accuracy and F1 score
fig, ax = plt.subplots(figsize=(15, 10))

x = results['Algorithm']
y1 = results['Accuracy Test']
y2 = results['F1 Test']

width = 0.35
x_pos = np.arange(len(x))

ax.bar(x_pos - width/2, y1, width, label='Test Accuracy')
ax.bar(x_pos + width/2, y2, width, label='Test F1 Score')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Scores')
ax.set_title('Classification Summary of Algorithms')
ax.set_xticks(x_pos)
ax.set_xticklabels(x, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()
```

**Training and Prediction Time Comparison:**

```python
fig, ax = plt.subplots(figsize=(15, 10))

x = results['Algorithm']
y1 = results['Training Time']
y2 = results['Prediction Time']

width = 0.35
x_pos = np.arange(len(x))

ax.bar(x_pos - width/2, y1, width, label='Training Time')
ax.bar(x_pos + width/2, y2, width, label='Prediction Time')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Time (seconds)')
ax.set_title('Training and Prediction Time of Algorithms')
ax.set_xticks(x_pos)
ax.set_xticklabels(x, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()
```

---

## HYPERPARAMETER TUNING

### Grid Search CV for Best Model

**Purpose:** Find optimal parameters for LinearSVC

```python
from sklearn.model_selection import GridSearchCV

def tuning(clf, param_dict, X_train, y_train, X_test, y_test):
    # Create scorer
    scorer = make_scorer(f1_score)
    
    # Grid search
    grid_object = GridSearchCV(
        estimator=clf,
        param_grid=param_dict,
        scoring=scorer,
        cv=5  # 5-fold cross-validation
    )
    
    # Fit grid search
    grid_fit = grid_object.fit(X_train, y_train)
    
    # Best estimator
    best_classifier = grid_fit.best_estimator_
    
    # Predictions
    prediction = best_classifier.fit(X_train, y_train).predict(X_train)
    best_prediction = best_classifier.predict(X_test)
    
    # Print results
    print(best_classifier.__class__.__name__)
    print(f"Best Parameters: {grid_fit.best_params_}")
    print(f"Accuracy: {accuracy_score(y_test, best_prediction):.2f}")
    print(f"F1 Score: {f1_score(y_test, best_prediction):.2f}")
    print(f"Precision: {precision_score(y_test, best_prediction):.2f}")
    print(f"Recall: {recall_score(y_test, best_prediction):.2f}")
    
    return best_classifier

# Define parameters to test
param_grid = {
    'C': [0.25, 0.5, 0.75, 1, 1.2]
}

# Run tuning
classifier_model = LinearSVC()

best_model = tuning(
    clf=classifier_model,
    param_dict=param_grid,
    X_train=training_data,
    y_train=y_train,
    X_test=testing_data,
    y_test=y_test
)
```

**Results:**
- **Best Parameter:** C = 1.2
- **Test Accuracy:** 96%
- **F1 Score:** 97%
- **Precision:** 97%
- **Recall:** 97%

**Improvement:** +3% accuracy, +4% F1 score after tuning

---

### Save Tuned Model

```python
pickle.dump(
    best_model,
    open('LinearSVC_tuned.pkl', 'wb')
)
```

---

## PREDICTION SYSTEM

### Create Prediction Function

```python
# Load saved vectorizer and model
tfidf_vectorizer = TfidfVectorizer(
    stop_words=content_list,
    lowercase=True,
    vocabulary=pickle.load(open('tfidf_vectorizer.pkl', 'rb'))
)

# Load trained model
train_model = pickle.load(open('LinearSVC.pkl', 'rb'))

# Test prediction
data = "Your test text here"

# Preprocess
preprocessed_data = tfidf_vectorizer.fit_transform([data])

# Predict
prediction = train_model.predict(preprocessed_data)

# Display result
if prediction == 1:
    print("Bullying")
else:
    print("Non-Bullying")
```

**Test Examples:**

**Example 1:** Bullying text
```python
data = df['headline'].iloc[0]  # Copy from dataset
# Output: "Bullying"
```

**Example 2:** Non-bullying text
```python
data = "You are so beautiful"
# Output: "Non-Bullying"
```

**Example 3:** Positive text
```python
data = "He is a good boy"
# Output: "Non-Bullying"
```

---

## FLASK WEB APPLICATION

### Project Structure

```
project/
â”‚
â”œâ”€â”€ app.py                    # Flask server
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Frontend
â”œâ”€â”€ LinearSVC.pkl            # Trained model
â”œâ”€â”€ LinearSVC_tuned.pkl      # Tuned model
â”œâ”€â”€ tfidf_vectorizer.pkl     # Vectorizer
â””â”€â”€ stopwords.txt            # Stop words list
```

---

### Flask Backend (app.py)

```python
from flask import Flask, render_template, request
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize Flask app
app = Flask(__name__)

# Load stopwords
with open('stopwords.txt', 'r') as file:
    content = file.read()
    content_list = content.split('\n')
    file.close()

# Load vectorizer
tfidf_vectorizer = TfidfVectorizer(
    stop_words=content_list,
    lowercase=True,
    vocabulary=pickle.load(open('tfidf_vectorizer.pkl', 'rb'))
)

# Load trained model
model = pickle.load(open('LinearSVC.pkl', 'rb'))
# Alternative: Use tuned model
# model = pickle.load(open('LinearSVC_tuned.pkl', 'rb'))

@app.route('/', methods=['GET', 'POST'])
def index():
    prediction = None
    
    if request.method == 'POST':
        # Get user input
        text = request.form['text']
        
        # Vectorize
        vectorized = tfidf_vectorizer.fit_transform([text])
        
        # Predict
        prediction = model.predict(vectorized)[0]
    
    return render_template('index.html', prediction=prediction)

if __name__ == '__main__':
    app.run(debug=True)
```

---

### Frontend (index.html)

```html
<!DOCTYPE html>
<html>
<head>
    <title>Cyberbullying Detection using Python</title>
    
    <!-- Bootstrap CSS CDN -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <a class="navbar-brand" href="#">Cyberbullying Detection using Python</a>
        <div class="collapse navbar-collapse">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item"><a class="nav-link" href="#">Home</a></li>
                <li class="nav-item"><a class="nav-link" href="#">About</a></li>
                <li class="nav-item"><a class="nav-link" href="#">Contact</a></li>
            </ul>
        </div>
    </nav>
    
    <!-- Main Container -->
    <div class="container mt-5">
        <h1>Cyberbullying Detection using Python</h1>
        
        <!-- Form -->
        <form method="POST">
            <div class="form-group">
                <textarea 
                    name="text" 
                    class="form-control" 
                    rows="5" 
                    placeholder="Enter text to analyze"
                    required
                ></textarea>
            </div>
            
            <button type="submit" class="btn btn-primary">Analyze</button>
        </form>
        
        <!-- Result Display -->
        {% if prediction is not none %}
            {% if prediction == 1 %}
                <div class="alert alert-danger mt-3">
                    <h3>Bullying</h3>
                </div>
            {% else %}
                <div class="alert alert-success mt-3">
                    <h3>Non-Bullying</h3>
                </div>
            {% endif %}
        {% endif %}
    </div>
    
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
```

---

### How Flask Works

**Flow:**

1. **GET Request (Initial Load):**
   ```
   User opens http://127.0.0.1:5000/
   â†’ Flask serves index.html
   â†’ Form displayed (prediction = None)
   ```

2. **POST Request (Form Submission):**
   ```
   User enters text and clicks "Analyze"
   â†’ Flask receives POST request
   â†’ Extract text from form
   â†’ Vectorize text
   â†’ Model predicts
   â†’ Return result to template
   â†’ Display colored alert (red/green)
   ```

**Jinja2 Template Logic:**
```python
{% if prediction is not none %}
    # Prediction exists
    {% if prediction == 1 %}
        # Show red alert: "Bullying"
    {% else %}
        # Show green alert: "Non-Bullying"
    {% endif %}
{% endif %}
```

---

### Running the Application

**Command:**
```bash
python app.py
```

**Output:**
```
* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```

**Access:** Open browser to http://127.0.0.1:5000/

**127.0.0.1 Explanation:**
- Loopback IP address (localhost)
- Class A IP address
- Only accessible from your machine

---

## KEY CONCEPTS EXPLAINED

### Text Preprocessing Pipeline

**Complete Flow:**

```
Raw Text
    â†“
Remove @ mentions (@user)
    â†“
Remove special characters (keep letters, #)
    â†“
Convert to lowercase
    â†“
Remove words < 3 characters
    â†“
Tokenization (split into words)
    â†“
Remove stop words (the, is, a, an)
    â†“
Lemmatization (running â†’ run)
    â†“
Join back to string
    â†“
Clean Text (ready for vectorization)
```

**Example:**
```
Input: "@user This is #BAD!!! I hate you!!!"
Output: "hate"
```

### TF-IDF Vectorization

**Why convert text to numbers?**
- Machine learning models need numerical input
- Text must be represented as feature vectors

**Bag of Words (Simple Count):**
```
"I hate you" â†’ [1, 1, 1] (count of each word)
```

**TF-IDF (Weighted Count):**
```
"I hate you" â†’ [0.1, 0.9, 0.3]
```
- Common words (I, you) get lower weight
- Distinctive words (hate) get higher weight

**Formula:**
```
TF-IDF(word) = TF(word) Ã— IDF(word)

TF = Word frequency in document
IDF = log(Total docs / Docs containing word)
```

### Classification Metrics

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
Percentage of correct predictions

**Precision:**
```
Precision = TP / (TP + FP)
```
Of predicted bullying, how many are actually bullying?

**Recall:**
```
Recall = TP / (TP + FN)
```
Of actual bullying, how many did we catch?

**F1 Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
Harmonic mean (balanced measure)

**For this problem:**
- **High Recall important:** Don't miss actual bullying
- **High Precision important:** Don't flag safe content
- **F1 Score:** Best overall metric for imbalanced data

### Grid Search CV

**Purpose:** Find best hyperparameters automatically

**How it works:**
```python
param_grid = {'C': [0.25, 0.5, 0.75, 1, 1.2]}
```

**Process:**
1. Try C=0.25 with 5-fold CV â†’ Average F1 score
2. Try C=0.5 with 5-fold CV â†’ Average F1 score
3. Try C=0.75 with 5-fold CV â†’ Average F1 score
4. Try C=1 with 5-fold CV â†’ Average F1 score
5. Try C=1.2 with 5-fold CV â†’ Average F1 score (Best!)

**Cross-Validation (CV=5):**
- Split training data into 5 folds
- Train on 4 folds, validate on 1
- Rotate and repeat 5 times
- Average results

**Benefit:** Prevents overfitting, finds robust parameters

---

## REAL-WORLD APPLICATION

### Use Cases

**1. Social Media Platforms:**
- Automatically flag bullying comments
- Warn users before posting harmful content
- Reduce moderator workload

**2. Educational Institutions:**
- Monitor student communications
- Early intervention for cyberbullying
- Protect student mental health

**3. Parental Control Apps:**
- Monitor children's online activity
- Alert parents to concerning messages
- Protect minors from harassment

**4. Corporate HR:**
- Monitor workplace communication channels
- Ensure respectful workplace environment
- Prevent harassment lawsuits

### Ethical Considerations

**Privacy Concerns:**
- Who has access to monitored data?
- How long is data stored?
- Transparency with users?

**False Positives:**
- Safe content flagged as bullying
- Context misunderstanding
- Sarcasm/jokes detected incorrectly

**False Negatives:**
- Actual bullying missed
- New bullying tactics not recognized
- Coded language not detected

**Best Practices:**
1. **Human review:** AI flags, humans decide
2. **Transparency:** Inform users of monitoring
3. **Appeal process:** Allow users to contest flags
4. **Continuous improvement:** Retrain on new data
5. **Privacy:** Encrypt data, limit access

---

## IMPROVEMENTS & EXTENSIONS

### Model Improvements

**1. Try Deep Learning:**
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential([
    Embedding(vocab_size, 128),
    LSTM(64),
    Dense(1, activation='sigmoid')
])
```

**Benefits:**
- Better context understanding
- Captures word order
- Higher accuracy potential

**2. Use Pre-trained Models:**
```python
from transformers import BertTokenizer, BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
```

**Benefits:**
- Transfer learning from massive datasets
- State-of-the-art performance
- Understands complex language

**3. Ensemble Methods:**
```python
# Combine multiple models
predictions = (
    0.3 * model1.predict(X) +
    0.4 * model2.predict(X) +
    0.3 * model3.predict(X)
)
```

**Benefits:**
- More robust predictions
- Reduced overfitting
- Better generalization

### Feature Engineering

**1. Add Sentiment Analysis:**
```python
from textblob import TextBlob

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

df['sentiment'] = df['text'].apply(get_sentiment)
```

**2. Count Features:**
```python
df['num_caps'] = df['text'].str.count(r'[A-Z]')  # All caps words
df['num_exclamations'] = df['text'].str.count('!')
df['num_questions'] = df['text'].str.count('\?')
```

**3. Word Embeddings:**
```python
from gensim.models import Word2Vec

# Train Word2Vec
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# Get vector for word
vector = model.wv['hate']
```

### Application Enhancements

**1. Multi-class Classification:**
```
Instead of: Bullying / Non-Bullying

Use: Hate Speech / Offensive / Neutral
```

**2. Severity Scoring:**
```python
# Rate bullying severity 1-10
def severity_score(text):
    # Your logic here
    return score  # 1-10
```

**3. Real-time Monitoring:**
```python
# Stream Twitter API
import tweepy

stream = tweepy.Stream(auth, listener)
stream.filter(track=['keyword'])

# For each tweet, check if bullying
for tweet in stream:
    prediction = model.predict(tweet.text)
    if prediction == 1:
        alert_moderators(tweet)
```

**4. Multi-language Support:**
```python
from googletrans import Translator

translator = Translator()

# Translate to English
translated = translator.translate(text, dest='en').text

# Then predict
prediction = model.predict(translated)
```

**5. User Dashboard:**
- Show user their own flagged messages
- Provide feedback mechanism
- Display toxicity trends over time

---

## COMMON ISSUES & SOLUTIONS

### Issue 1: NLTK Data Not Downloaded

**Error:** `LookupError: Resource wordnet not found`

**Solution:**
```python
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
```

### Issue 2: Model Doesn't Load

**Error:** `FileNotFoundError: model.pkl not found`

**Solution:**
- Check file is in same directory as app.py
- Verify filename exactly matches
- Check file permissions

### Issue 3: Vectorizer Vocabulary Mismatch

**Error:** `KeyError: 'word' not in vocabulary`

**Solution:**
- Use same vectorizer for training and prediction
- Load saved vectorizer vocabulary
- Don't create new vectorizer in prediction

### Issue 4: Low Accuracy

**Problem:** Model only getting 60-70% accuracy

**Solutions:**
1. **More data:** Collect more training samples
2. **Better cleaning:** Improve text preprocessing
3. **Feature engineering:** Add more features
4. **Try different model:** Test deep learning
5. **Hyperparameter tuning:** Use Grid Search

### Issue 5: Flask Not Running

**Error:** `Address already in use`

**Solution:**
```bash
# Kill process on port 5000
lsof -ti:5000 | xargs kill -9

# Or use different port
app.run(port=5001)
```

---

## BEST PRACTICES

### Data Science

**1. Always Split Data:**
- Never train and test on same data
- Use separate test set for final evaluation
- Consider validation set for hyperparameter tuning

**2. Cross-Validation:**
- Don't trust single train-test split
- Use k-fold cross-validation
- Report average and standard deviation

**3. Track Experiments:**
```python
import mlflow

mlflow.log_param("model", "LinearSVC")
mlflow.log_param("C", 1.2)
mlflow.log_metric("accuracy", 0.96)
```

**4. Version Control:**
- Use Git for code
- Tag model versions
- Document experiments in README

### Production Deployment

**1. Error Handling:**
```python
@app.route('/', methods=['POST'])
def predict():
    try:
        text = request.form['text']
        prediction = model.predict([text])
        return jsonify({'result': int(prediction[0])})
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**2. Input Validation:**
```python
if not text or len(text) < 3:
    return "Text too short", 400

if len(text) > 5000:
    return "Text too long", 400
```

**3. Rate Limiting:**
```python
from flask_limiter import Limiter

limiter = Limiter(app, key_func=lambda: request.remote_addr)

@limiter.limit("10 per minute")
@app.route('/predict')
def predict():
    # Your code
```

**4. Monitoring:**
- Log all predictions
- Track accuracy over time
- Alert on model drift

---

## CONCLUSION

### What Was Accomplished

**Technical:**
- âœ… Text preprocessing pipeline (regex, lemmatization, stopwords)
- âœ… TF-IDF vectorization
- âœ… Trained 7 classification algorithms
- âœ… Hyperparameter tuning (Grid Search CV)
- âœ… Model evaluation (accuracy, precision, recall, F1)
- âœ… Model persistence (pickle files)
- âœ… Flask web application deployment

**Results:**
- **Best Model:** DecisionTreeClassifier (~98% accuracy)
- **Best Speed/Accuracy:** LinearSVC (96% accuracy, 0.34s training)
- **After Tuning:** LinearSVC (97% F1 score)

### Key Learnings

1. **Text preprocessing is critical:** Garbage in = Garbage out
2. **Multiple models matter:** Test several algorithms
3. **Tuning improves performance:** +3-4% accuracy gain
4. **Speed vs accuracy trade-off:** LinearSVC best balance
5. **F1 score important:** Better than accuracy for imbalanced data

### Real-World Impact

**Potential Benefits:**
- Reduce cyberbullying incidents
- Early intervention for at-risk youth
- Safer online communities
- Reduced moderation workload

**Limitations:**
- Context understanding (sarcasm, jokes)
- New bullying tactics (coded language)
- Privacy concerns
- False positives/negatives

### Next Steps

**For Beginners:**
1. Understand the code thoroughly
2. Test with different texts
3. Modify parameters and observe changes
4. Try adding new features

**For Advanced Users:**
1. Implement deep learning (LSTM, BERT)
2. Multi-class classification (hate speech types)
3. Real-time monitoring system
4. Deploy to cloud (AWS, Azure, Heroku)
5. Create mobile app integration

---

*This project demonstrates practical application of NLP and machine learning to combat cyberbullying - a critical social issue affecting millions of young people worldwide. While no system is perfect, AI can be a powerful tool to make online spaces safer.*
